[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unsupervised Machine Learning",
    "section": "",
    "text": "Preface\nThese notes are a personal reference related to unsupervised machine learning (ML).\nRepo for this quarto book: https://github.com/mpfoley73/unsupervised-ml.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-cluster-analysis.html",
    "href": "01-cluster-analysis.html",
    "title": "1  Cluster Analysis",
    "section": "",
    "text": "1.1 Case Study\nLet’s learn by example, using the IBM HR Analytics Employee Attrition & Performance data set from Kaggle to discover which factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover.1 Data set eap includes 1,470 employee records consisting of the EmployeeNumber, a flag for Attrition (Yes|No) during an (unspecified) time frame, and 32 other descriptive variables.\nlibrary(tidyverse)\nlibrary(plotly)            # interactive graphing\nlibrary(cluster)           # daisy and pam\nlibrary(Rtsne)             # dimensionality reduction and visualization\nlibrary(dendextend)        # color_branches\n\n# set.seed(1234)  # reproducibility\n\neap_0 &lt;- read_csv(\"./input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\neap_1 &lt;- eap_0 %&gt;%\n  mutate_if(is.character, as_factor) %&gt;%\n  mutate(\n    EnvironmentSatisfaction = factor(EnvironmentSatisfaction, ordered = TRUE),\n    StockOptionLevel = factor(StockOptionLevel, ordered = TRUE),\n    JobLevel = factor(JobLevel, ordered = TRUE),\n    JobInvolvement = factor(JobInvolvement, ordered = TRUE)\n  ) %&gt;%\n  select(EmployeeNumber, Attrition, everything())\n\nmy_skim &lt;- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))\nmy_skim(eap_1)\n\n\nData summary\n\n\nName\neap_1\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n13\n\n\nnumeric\n22\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAttrition\n0\n1\nFALSE\n2\nNo: 1233, Yes: 237\n\n\nBusinessTravel\n0\n1\nFALSE\n3\nTra: 1043, Tra: 277, Non: 150\n\n\nDepartment\n0\n1\nFALSE\n3\nRes: 961, Sal: 446, Hum: 63\n\n\nEducationField\n0\n1\nFALSE\n6\nLif: 606, Med: 464, Mar: 159, Tec: 132\n\n\nEnvironmentSatisfaction\n0\n1\nTRUE\n4\n3: 453, 4: 446, 2: 287, 1: 284\n\n\nGender\n0\n1\nFALSE\n2\nMal: 882, Fem: 588\n\n\nJobInvolvement\n0\n1\nTRUE\n4\n3: 868, 2: 375, 4: 144, 1: 83\n\n\nJobLevel\n0\n1\nTRUE\n5\n1: 543, 2: 534, 3: 218, 4: 106\n\n\nJobRole\n0\n1\nFALSE\n9\nSal: 326, Res: 292, Lab: 259, Man: 145\n\n\nMaritalStatus\n0\n1\nFALSE\n3\nMar: 673, Sin: 470, Div: 327\n\n\nOver18\n0\n1\nFALSE\n1\nY: 1470\n\n\nOverTime\n0\n1\nFALSE\n2\nNo: 1054, Yes: 416\n\n\nStockOptionLevel\n0\n1\nTRUE\n4\n0: 631, 1: 596, 2: 158, 3: 85\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np100\n\n\n\n\nEmployeeNumber\n0\n1\n1024.87\n602.02\n1\n2068\n\n\nAge\n0\n1\n36.92\n9.14\n18\n60\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n1499\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n29\n\n\nEducation\n0\n1\n2.91\n1.02\n1\n5\n\n\nEmployeeCount\n0\n1\n1.00\n0.00\n1\n1\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n100\n\n\nJobSatisfaction\n0\n1\n2.73\n1.10\n1\n4\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n19999\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n26999\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n9\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n25\n\n\nPerformanceRating\n0\n1\n3.15\n0.36\n3\n4\n\n\nRelationshipSatisfaction\n0\n1\n2.71\n1.08\n1\n4\n\n\nStandardHours\n0\n1\n80.00\n0.00\n80\n80\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n40\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n6\n\n\nWorkLifeBalance\n0\n1\n2.76\n0.71\n1\n4\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n40\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n18\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n15\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n17",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "01-cluster-analysis.html#case-study",
    "href": "01-cluster-analysis.html#case-study",
    "title": "1  Cluster Analysis",
    "section": "",
    "text": "Data Exploration\nYou would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I’ll bypass that rigor and just use a binary correlation analysis using the correlationfunnel package. binarize() converts features into binary format by binning the continuous features and one-hot encoding the binary features. correlate() calculates the correlation coefficient between each binary feature and the response variable. plot_correlation_funnel() creates a tornado plot that lists the highest correlation features (based on absolute magnitude) at the top.\n\ncf &lt;- eap_1 %&gt;%\n  select(-EmployeeNumber) %&gt;%\n  correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %&gt;%\n  correlationfunnel::correlate(Attrition__Yes)\n\ncf %&gt;%\n  correlationfunnel::plot_correlation_funnel(interactive = FALSE) %&gt;%\n  ggplotly()\n\n\n\n\n\nOverTime (Y|N) has the largest correlation (0.25). I’ll include just the variables with a correlation coefficient of at least 0.10.\n\nfeature_cols &lt;- cf %&gt;% \n  filter(abs(correlation) &gt;= .1 & feature != \"Attrition\") %&gt;%\n  pull(feature) %&gt;%\n  as.character() %&gt;%\n  unique()\n\neap_2 &lt;- eap_1 %&gt;% \n  select(one_of(c(\"EmployeeNumber\", \"Attrition\", feature_cols)))\n\nUsing the cutoff of 0.10 leaves 14 features for the analysis.\n\neap_2 %&gt;%\n  select(-EmployeeNumber) %&gt;%\n  correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %&gt;%\n  correlationfunnel::correlate(Attrition__Yes) %&gt;%\n  correlationfunnel::plot_correlation_funnel(interactive = FALSE) %&gt;%\n  ggplotly()\n\n\n\n\n\n\n\nData Preparation\nMeasuring distance is central to clustering. Two observations are similar if the distance between their features is small. There are many ways to define distance (see options in ?dist). Two common measures are Euclidean distance (\\(d = \\sqrt{\\sum{(x_i - y_i)^2}}\\)), and Jaccard distance (proportion of unshared features). If you have a mix of feature data types, use the Gower distance. Gower range-normalizes the quantitative variables, one-hot encodes the nominal variables, and ranks the ordinal variables, then calculates the Manhattan distance for quantitative and ordinal variables and the Dice coefficient for nominal variables. calculate Gower’s distance with cluster::daisy().\n\neap_2_gwr &lt;- cluster::daisy(eap_2[, 2:16], metric = \"gower\")\n\nLet’s see the most similar and dissimilar pairs of employees according to their Gower distance. The most similar pair are identical except for MonthlyIncome. The most dissimilar pair have nothing in common.\n\nx &lt;- as.matrix(eap_2_gwr)\nbind_rows(\n  eap_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ],\n  eap_2[which(x == max(x[x != 0]), arr.ind = TRUE)[1, ], ]\n) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(across(everything(), as.character)) %&gt;%\n  pivot_longer(cols = -EmployeeNumber) %&gt;%\n  pivot_wider(names_from = EmployeeNumber) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::add_header_row(values = c(\"\", \"Similar\", \"Dissimilar\"), colwidths = c(1, 2, 2)) %&gt;%\n  flextable::border(j = c(1, 3), border.right = officer::fp_border(\"gray80\"), part = \"all\")\n\nSimilarDissimilarname16246141094825AttritionYesYesNoYesOverTimeYesYesNoYesJobLevel1115MonthlyIncome15691878462119246YearsAtCompany00331StockOptionLevel0030YearsWithCurrManager0028TotalWorkingYears00340MaritalStatusSingleSingleMarriedSingleAge18182758YearsInCurrentRole00215JobRoleSales RepresentativeSales RepresentativeLaboratory TechnicianResearch DirectorEnvironmentSatisfaction2214JobInvolvement3313BusinessTravelTravel_FrequentlyTravel_FrequentlyNon-TravelTravel_Rarely\nMost similar and dissimilar employees.\n\n\nWith the data preparation complete, we are ready to fit a clustering model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "01-cluster-analysis.html#k-means-medoids",
    "href": "01-cluster-analysis.html#k-means-medoids",
    "title": "1  Cluster Analysis",
    "section": "1.2 K-Means (Medoids)",
    "text": "1.2 K-Means (Medoids)\nThe K-means algorithm randomly assigns all observations to one of K clusters. K-means iteratively calculates the cluster centroids and reassigns observations to their nearest centroid. Centroids are set of mean values for each feature (hence the name “K-means”). The iterations continue until either the centroids stabilize or the iterations reach a set maximum (typically 50). The result is K clusters with the minimum total intra-cluster variation.\nThe centroid of cluster \\(c_i \\in C\\) is the mean of the cluster observations \\(S_i\\): \\(c_i = \\frac{1}{|S_i|} \\sum_{x_i \\in S_i}{x_i}\\). The nearest centroid is the minimum squared euclidean distance, \\(\\underset{c_i \\in C}{\\operatorname{arg min}} D(c_i, x)^2\\). A more robust version of K-means is K-medoids which minimizes the sum of dissimilarities instead of a sum of squared euclidean distances.\n\n1.2.0.1 Fitting the Model\nWhat value should k take? You may have a preference in advance, but more likely you will use either a scree plot (K-means) or the silhouette plot (K-medoids).\nI’ll construct a scree plot for reference, but I think K-medoids and silhouette plots are the newer, better way to cluster. The scree plot is a plot of the total within-cluster sum of squared distances as a function of K. The sum of squares always decreases as K increases, but at a declining rate. The optimal K is at the “elbow” in the curve - the point at which the curve flattens. In the scree plot below, the elbow may be K = 5.\n\nset.seed(1234)\n\nkmeans_mdl &lt;- data.frame(k = 2:10) %&gt;%\n  mutate(\n    mdl = map(k, ~stats::kmeans(eap_2_gwr, centers = .)),\n    wss = map_dbl(mdl, ~ .$tot.withinss)\n  )\n\nkmeans_mdl %&gt;%\n  ggplot(aes(x = k, y = wss)) +\n  geom_point(size = 2) +\n  geom_line() +\n  geom_vline(aes(xintercept = 5), linetype = 2, size = 1, color = \"goldenrod\") +\n  scale_x_continuous(breaks = kmeans_mdl$k) +\n  theme_light() +\n  labs(title = \"Scree plot elbow occurs at K = 5 clusters.\", \n       subtitle = \"K-Means within-cluster sum of squared distances at candidate values of K.\", \n       y = \"\")\n\n\n\n\n\n\n\n\nThe silhouette method calculates the within-cluster distance \\(C(i)\\) for each observation, and its distance to the nearest cluster \\(N(i)\\). The silhouette width is\n\\[\n\\begin{align}\nS &= \\frac{C(i)}{N(i)} - 1, & C(i) &gt; N(i)\\\\\n  &= 1 - \\frac{C(i)}{N(i)}, & C(i) &lt; N(i).\n\\end{align}\n\\]\nA value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster. The optimal number of clusters is the number that maximizes the total silhouette width. cluster::pam() returns a list in which one of the components is the average width silinfo$avg.width. In the silhouette plot below, the maximum width is at k = 6.\n\nset.seed(1234)\n\npam_mdl &lt;- data.frame(k = 2:10) %&gt;%\n  mutate(\n    mdl = map(k, ~pam(eap_2_gwr, k = .)),\n    sil = map_dbl(mdl, ~ .$silinfo$avg.width)\n  )\n\npam_mdl %&gt;%\n  ggplot(aes(x = k, y = sil)) +\n  geom_point(size = 2) +\n  geom_line() +\n  geom_vline(aes(xintercept = 6), linetype = 2, size = 1, color = \"goldenrod\") +\n  scale_x_continuous(breaks = pam_mdl$k) +\n  theme_light() +\n  labs(title = \"Silhouette plot max occurs at K = 6 clusters.\", \n       subtitle = \"K-Medoids within-cluster average silhouette width at candidate values of K.\", \n       y = \"\")\n\n\n\n\n\n\n\n\n\n\nSummarize Results\nAttach the results to the original table for visualization and summary statistics.\n\npam_mdl_final &lt;- pam_mdl %&gt;% filter(k == 6) %&gt;% pluck(\"mdl\", 1)\n\neap_3 &lt;- eap_2 %&gt;% \n  mutate(cluster = as.factor(pam_mdl_final$clustering))\n\nHere are the six medoids observations.\n\neap_3[pam_mdl_final$medoids, ] %&gt;%\n  t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% \n  flextable::flextable() %&gt;% flextable::autofit()\n\nrownameV1V2V3V4V5V6EmployeeNumber1171  35  65 221 7471408AttritionNoNoYesNoNoNoOverTimeNoNoYesNoNoNoJobLevel221124MonthlyIncome 5155 6825 3441 2713 530416799YearsAtCompany 6 9 2 5 820StockOptionLevel010111YearsWithCurrManager522279TotalWorkingYears 910 2 51021MaritalStatusSingleMarriedSingleMarriedDivorcedMarriedAge424228283042YearsInCurrentRole472277JobRoleSales ExecutiveSales ExecutiveLaboratory TechnicianResearch ScientistSales ExecutiveManagerEnvironmentSatisfaction233333JobInvolvement333333BusinessTravelTravel_RarelyTravel_RarelyTravel_RarelyTravel_RarelyTravel_RarelyTravel_Rarelycluster123456\n\n\nWhat can the medoids tell us about attrition?. Do high-attrition employees fall into a particular cluster? Yes! 79.7% of cluster 3 left the company. - that’s 59.5% of all turnover in the company.\n\neap_3_smry &lt;- eap_3 %&gt;%\n  count(cluster, Attrition) %&gt;%\n  group_by(cluster) %&gt;%\n  mutate(cluster_n = sum(n),\n         turnover_rate = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition == \"Yes\") %&gt;%\n  mutate(pct_of_turnover = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;%\n  select(cluster, cluster_n, turnover_n = n, turnover_rate, pct_of_turnover)\n\neap_3_smry %&gt;% flextable::flextable() %&gt;% flextable::autofit()\n\nclustercluster_nturnover_nturnover_ratepct_of_turnover1268238.6%9.7%2280248.6%10.1%317714179.7%59.5%4364298.0%12.2%520284.0%3.4%6179126.7%5.1%\n\n\nYou can get some sense of the quality of clustering by constructing the Barnes-Hut t-Distributed Stochastic Neighbor Embedding (t-SNE).\n\neap_4 &lt;- eap_3 %&gt;%\n  left_join(eap_3_smry, by = \"cluster\") %&gt;%\n  rename(Cluster = cluster) %&gt;%\n  mutate(\n    MonthlyIncome = MonthlyIncome %&gt;% scales::dollar(),\n    description = str_glue(\"Turnover = {Attrition}\n                                  MaritalDesc = {MaritalStatus}\n                                  Age = {Age}\n                                  Job Role = {JobRole}\n                                  Job Level {JobLevel}\n                                  Overtime = {OverTime}\n                                  Current Role Tenure = {YearsInCurrentRole}\n                                  Professional Tenure = {TotalWorkingYears}\n                                  Monthly Income = {MonthlyIncome}\n                                 \n                                  Cluster: {Cluster}\n                                  Cluster Size: {cluster_n}\n                                  Cluster Turnover Rate: {turnover_rate}\n                                  Cluster Turnover Count: {turnover_n}\n                                  \"))\n\ntsne_obj &lt;- Rtsne(eap_2_gwr, is_distance = TRUE)\n\ntsne_tbl &lt;- tsne_obj$Y %&gt;%\n  as_tibble() %&gt;%\n  setNames(c(\"X\", \"Y\")) %&gt;%\n  bind_cols(eap_4) %&gt;%\n  mutate(Cluster = as_factor(Cluster))\n\ng &lt;- tsne_tbl %&gt;%\n  ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) +\n  geom_point()\n\nggplotly(g)\n\n\n\n\n\nAnother approach is to take summary statistics and draw your own conclusions. You might start by asking which attributes differ among the clusters. The box plots below show the distribution of the numeric variables. All of the numeric variable distributions appear to vary among the clusters.\n\nmy_boxplot &lt;- function(y_var){\n  eap_3 %&gt;%\n    ggplot(aes(x = cluster, y = !!sym(y_var))) +\n    geom_boxplot() +\n    geom_jitter(aes(color = Attrition), alpha = 0.2, height = 0.10) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    labs(x = \"\", y = \"\", title = y_var)\n}\nvars_numeric &lt;- eap_3 %&gt;% select(-EmployeeNumber) %&gt;% select_if(is.numeric) %&gt;% colnames()\ng &lt;- map(vars_numeric, my_boxplot)\ngridExtra::marrangeGrob(g, nrow=1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can perform an analysis of variance to confirm. The table below collects the ANOVA results for each of the numeric variables. The results indicate that there are significant differences among clusters at the .01 level for all of the numeric variables.\n\nkm_aov &lt;- vars_numeric %&gt;% \n  map(~ aov(rlang::eval_tidy(expr(!!sym(.x) ~ cluster)), data = eap_3))\nkm_aov %&gt;% \n  map(anova) %&gt;%\n  map(~ data.frame(F = .x$`F value`[[1]], p = .x$`Pr(&gt;F)`[[1]])) %&gt;%\n  bind_rows() %&gt;%\n  bind_cols(Attribute = vars_numeric) %&gt;%\n  select(Attribute, everything()) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::colformat_double(j = 2, digits = 2) %&gt;%\n  flextable::colformat_double(j = 3, digits = 4) %&gt;%\n  flextable::autofit()\n\nAttributeFpMonthlyIncome718.880.0000YearsAtCompany134.580.0000YearsWithCurrManager65.530.0000TotalWorkingYears342.880.0000Age98.760.0000YearsInCurrentRole69.530.0000\n\n\nWhat sets Cluster 3, the high attrition cluster, apart from the others? Cluster 3 is similar to cluster 4 in almost all attributes. They tend to have the smaller incomes, company and job tenure, years with their current manager, total working experience, and age. I.e., they tend to be lower on the career ladder.\nTo drill into cluster differences to determine which clusters differ from others, use the Tukey HSD post hoc test with Bonferroni method applied to control the experiment-wise error rate. That is, only reject the null hypothesis of equal means among clusters if the p-value is less than \\(\\alpha / p\\), or \\(.05 / 6 = 0.0083\\). The significantly different cluster combinations are shown in bold. Clusters 3 and 4 differ from the others on all six measures. However, there are no significant differences between c3 and c4 (highlighted green) for the numeric variables.\n\nkm_hsd &lt;- map(km_aov, TukeyHSD)\n\nmap(km_hsd, ~ .x$cluster %&gt;% \n      data.frame() %&gt;% \n      rownames_to_column() %&gt;% \n      filter(str_detect(rowname, \"-\"))) %&gt;% \n  map2(vars_numeric, bind_cols) %&gt;%\n  bind_rows() %&gt;% \n  select(predictor = `...6`, everything()) %&gt;%\n  mutate(cluster_a = str_sub(rowname, start = 1, end = 1),\n         cluster = paste0(\"c\", str_sub(rowname, start = 3, end = 3))) %&gt;%\n  pivot_wider(id_cols = c(predictor, cluster), \n              names_from = cluster_a, values_from = p.adj,\n              names_prefix = \"c\") %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::colformat_double(j = c(3:7), digits = 4) %&gt;%\n  flextable::bold(i = ~ c2 &lt; .05 / length(vars_numeric), j = ~ c2, bold = TRUE) %&gt;%\n  flextable::bold(i = ~ c3 &lt; .05 / length(vars_numeric), j = ~ c3, bold = TRUE) %&gt;%\n  flextable::bold(i = ~ c4 &lt; .05 / length(vars_numeric), j = ~ c4, bold = TRUE) %&gt;%\n  flextable::bold(i = ~ c5 &lt; .05 / length(vars_numeric), j = ~ c5, bold = TRUE) %&gt;%\n  flextable::bold(i = ~ c6 &lt; .05 / length(vars_numeric), j = ~ c6, bold = TRUE) %&gt;%\n  flextable::bg(i = ~ cluster == \"c3\", j = ~ c4, bg = \"#B6E2D3\") %&gt;%\n  flextable::border(i = ~ cluster == \"c1\", border.top = officer::fp_border()) %&gt;%\n  flextable::autofit() \n\npredictorclusterc2c3c4c5c6MonthlyIncomec10.65870.00000.00000.97200.0000MonthlyIncomec20.00000.00000.98990.0000MonthlyIncomec30.39750.00000.0000MonthlyIncomec40.00000.0000MonthlyIncomec50.0000YearsAtCompanyc10.99990.00000.00000.99220.0000YearsAtCompanyc20.00000.00000.99890.0000YearsAtCompanyc30.99350.00000.0000YearsAtCompanyc40.00000.0000YearsAtCompanyc50.0000YearsWithCurrManagerc11.00000.00000.00000.97630.0000YearsWithCurrManagerc20.00000.00000.95950.0000YearsWithCurrManagerc30.99650.00000.0000YearsWithCurrManagerc40.00000.0000YearsWithCurrManagerc50.0000TotalWorkingYearsc10.98400.00000.00000.99800.0000TotalWorkingYearsc20.00000.00000.89340.0000TotalWorkingYearsc30.99980.00000.0000TotalWorkingYearsc40.00000.0000TotalWorkingYearsc50.0000Agec10.99800.00000.00000.99890.0000Agec20.00000.00000.96910.0000Agec30.05320.00000.0000Agec40.00000.0000Agec50.0000YearsInCurrentRolec10.10130.00000.00000.64710.0000YearsInCurrentRolec20.00000.00000.95730.0000YearsInCurrentRolec30.98540.00000.0000YearsInCurrentRolec40.00000.0000YearsInCurrentRolec50.0000\n\n\nSo far the picture is incomplete. High-attrition employees are low on the career ladder, but cluster 4 is also low on the career ladder and they are not high-attrition.\nHow about the factor variables? The tile plots below show that clusters 3 and 4 are lab technicians and research scientists. They are both at the lowest job level. But three factors distinguish these clusters from each other: cluster 3 is far more likely to work overtime, have no stock options, and be single.\n\nmy_tileplot &lt;- function(y_var){\n  eap_3 %&gt;%\n    count(cluster, !!sym(y_var)) %&gt;%\n    ungroup() %&gt;%\n    group_by(cluster) %&gt;%\n    mutate(pct = n / sum(n)) %&gt;%\n    ggplot(aes(y = !!sym(y_var), x = cluster, fill = pct)) +\n    geom_tile() +\n    scale_fill_gradient(low = \"#E9EAEC\", high = \"#FAD02C\") +\n    geom_text(aes(label = scales::percent(pct, accuracy = 1.0)), size = 3) +\n    theme_minimal() + \n    theme(legend.position = \"none\")\n}\nvars_factor &lt;- eap_3 %&gt;% select(-cluster) %&gt;% select_if(is.factor) %&gt;% colnames()\n\ng &lt;- map(vars_factor, my_tileplot)\ngridExtra::marrangeGrob(g, nrow=1, ncol = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can perform a chi-squared independence test to confirm. The table below collects the Chi-Sq test results for each of the factor variables. The results indicate that there are significant differences among clusters at the .05 level for all of the factor variables except EnvironmentSatisfaction and JobInvolvement.\n\nkm_chisq &lt;- vars_factor %&gt;%\n  map(~ janitor::tabyl(eap_3, cluster, !!sym(.x))) %&gt;%\n  map(janitor::chisq.test)\n\nkm_chisq %&gt;% \n  map(~ data.frame(ChiSq = .x$statistic[[1]], df = .x$parameter[[1]], p = .x$p.value[[1]])) %&gt;%\n  bind_rows() %&gt;%\n  bind_cols(Attribute = vars_factor) %&gt;%\n  select(Attribute, everything()) %&gt;%\n  flextable::flextable() %&gt;%\n  flextable::colformat_double(j = ~ ChiSq, digits = 1) %&gt;%\n  flextable::colformat_double(j = ~ p, digits = 4) %&gt;%\n  flextable::autofit()\n\nAttributeChiSqdfpAttrition603.250.0000OverTime199.950.0000JobLevel1,855.6200.0000StockOptionLevel731.2150.0000MaritalStatus1,853.4100.0000JobRole1,754.0400.0000EnvironmentSatisfaction20.0150.1712JobInvolvement22.6150.0938BusinessTravel19.7100.0324\n\n\nWhat sets cluster 3 apart from the others, cluster 4 in particular? Cluster 3 is far more likely to work overtime, have no stock options, and be single. Perform a residuals analysis on the the chi-sq test to verify. The residuals with absolute value &gt;2 are driving the differences among the clusters.\n\nkm_chisq %&gt;%\n  map(~ data.frame(.x$residuals)) %&gt;%\n  map(data.frame) %&gt;%\n  map(t) %&gt;%\n  map(data.frame) %&gt;%\n  map(rownames_to_column, var = \"Predictor\") %&gt;%\n  map(~ filter(.x, Predictor != \"cluster\")) %&gt;%\n  map(~ select(.x, Predictor, c1 = X1, c2 = X2, c3 = X3, c4 = X4, c5 = X5, c6 = X6)) %&gt;%\n  map(~ mutate_at(.x, vars(2:7), as.numeric)) %&gt;%\n  map(flextable::flextable) %&gt;%\n  map(~ flextable::colformat_double(.x, j = ~ c1 + c2 + c3 + c4 + c5 + c6, digits = 1)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c1) &gt; 2, j = ~ c1, bold = TRUE)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c2) &gt; 2, j = ~ c2, bold = TRUE)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c3) &gt; 2, j = ~ c3, bold = TRUE)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c4) &gt; 2, j = ~ c4, bold = TRUE)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c5) &gt; 2, j = ~ c5, bold = TRUE)) %&gt;%\n  map(~ flextable::bold(.x, i = ~ abs(c6) &gt; 2, j = ~ c6, bold = TRUE)) %&gt;%\n#  map(~ flextable::bg(i = ~ cluster == \"c3\", j = ~ c4, bg = \"#B6E2D3\") %&gt;%\nmap2(vars_factor, ~ flextable::set_caption(.x, caption = .y))\n\n[[1]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 2 row(s) \noriginal dataset sample: \n  Predictor        c1        c2        c3        c4        c5        c6\n1       Yes -3.074284 -3.146800 21.052736 -3.875086 -4.304940 -3.138300\n2        No  1.347835  1.379627 -9.229989  1.698924  1.887382  1.375901\n\n[[2]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 2 row(s) \noriginal dataset sample: \n  Predictor        c1         c2        c3        c4          c5          c6\n1       Yes -4.000828 -0.5884457 10.725697 -3.449431 -0.15403618  0.04836361\n2        No  2.513485  0.3696858 -6.738324  2.167075  0.09677186 -0.03038401\n\n[[3]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 5 row(s) \noriginal dataset sample: \n  Predictor        c1        c2        c3        c4        c5         c6\n1        X1 -4.622859 -9.678341  9.599235 15.569991 -5.512378 -8.1314460\n2        X2  4.828776  9.745394 -5.150270 -7.324817  5.208898 -8.0637760\n3        X3  2.578522  4.729466 -3.561905 -7.211066  4.210211  0.2822892\n4        X4 -2.121266 -3.825733 -3.012750 -5.123243 -2.768472 20.6230820\n5        X5 -2.418986 -3.625308 -2.535454 -4.133487 -3.079226 19.1807890\n\n[[4]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 4 row(s) \noriginal dataset sample: \n  Predictor         c1          c2         c3         c4        c5         c6\n1        X0  14.261198 -6.03754600  7.6891380 -4.3398430 -8.667412 -2.1488560\n2        X1 -10.423939  4.83128800 -6.1104130  3.8210350  3.989103  3.1019730\n3        X2  -5.367070  2.71691560 -2.9860991  0.4598298  6.071044 -0.9665264\n4        X3  -3.936572 -0.04733811 -0.6985228  1.0794749  4.775144 -1.0413857\n\n[[5]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 3 row(s) \noriginal dataset sample: \n  Predictor         c1        c2        c3        c4        c5         c6\n1    Single  19.587144 -9.461702  9.492289 -6.338612 -8.036481 -3.9961330\n2   Married -10.986571 13.408220 -6.224744  6.611745 -9.616666  3.6508300\n3  Divorced  -7.721161 -7.892130 -2.450022 -1.886047 23.430922 -0.4466382\n\n[[6]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 9 row(s) \noriginal dataset sample: \n                  Predictor        c1        c2        c3        c4        c5\n1           Sales.Executive  4.224222 10.393935 -3.551837 -8.984643  3.616083\n2        Research.Scientist -3.047504 -7.055555  1.153688 15.017279 -3.808570\n3     Laboratory.Technician -0.468456 -3.037306  7.308603  2.730491 -1.272337\n4    Manufacturing.Director  2.249256  1.975299 -2.742469 -3.822520  3.601185\n5 Healthcare.Representative  1.865554  4.213539 -2.964428 -4.993130  3.299386\n          c6\n1 -5.6656560\n2 -5.9629240\n3 -5.4378120\n4 -0.8701804\n5 -0.9894197\n\n[[7]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 4 row(s) \noriginal dataset sample: \n  Predictor         c1         c2          c3          c4         c5         c6\n1        X1  1.4207444 -1.1006522  0.47951650 -0.39635550 -0.4843633  0.2410757\n2        X2  1.8906688  1.2623375 -1.28554920 -0.71964310 -0.7067058 -0.8369268\n3        X3 -2.0453569  0.7228204  0.06162142  0.83358388  0.2219346  0.1129351\n4        X4 -0.5890421 -0.8627985  0.58649848  0.05346915  0.7297500  0.3651772\n\n[[8]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 4 row(s) \noriginal dataset sample: \n  Predictor          c1         c2         c3          c4         c5         c6\n1        X1 -0.03392631 -0.7065995  1.8998844 -1.44533350  1.3604659 -0.3481475\n2        X2  0.56028040  0.4225771  0.5724950 -0.40027460 -1.6062743  0.4937853\n3        X3  0.21879629 -0.2592379 -0.3437552 -0.06366268  0.2494020  0.2241808\n4        X4 -1.41557010  0.4909903 -1.5222857  1.89954330  0.9469245 -1.0829262\n\n[[9]]\na flextable object.\ncol_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` \nheader has 1 row(s) \nbody has 3 row(s) \noriginal dataset sample: \n          Predictor         c1          c2          c3         c4          c5\n1     Travel_Rarely -1.0263102 -0.33108864 -0.05226557  0.6056579 -0.02704771\n2 Travel_Frequently  1.3367306  0.03277861  1.84355843 -0.9165098 -0.82079013\n3        Non.Travel  0.8897836  0.82850980 -2.36743050 -0.3516054  1.18671160\n          c6\n1  0.8869173\n2 -1.3309690\n3 -0.5300458\n\n\nCluster 3 are significantly more likely to work overtime while cluster 4 (and 1) are significantly less likely. Cluster 3 (and 1) are significantly more likely to have no stock options and be single.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "01-cluster-analysis.html#hca",
    "href": "01-cluster-analysis.html#hca",
    "title": "1  Cluster Analysis",
    "section": "1.3 HCA",
    "text": "1.3 HCA\nHierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters (usually presented in a dendrogram). The HCA process is:\n\nCalculate the distance between each observation with dist() or daisy(). We did that above when we created eap_2_gwr.\nCluster the two closest observations into a cluster with hclust(). Then calculate the cluster’s distance to the remaining observations. If the shortest distance is between two observations, define a second cluster, otherwise adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The “distance” to a cluster can be defined as:\n\n\ncomplete: distance to the furthest member of the cluster,\nsingle: distance to the closest member of the cluster,\naverage: average distance to all members of the cluster, or\ncentroid: distance between the centroids of each cluster.\n\nComplete and average distances tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. This is useful for identifying outliers.\n\nmdl_hc &lt;- hclust(eap_2_gwr, method = \"complete\")\n\n\nEvaluate the hclust tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Choose the number of clusters to keep by identifying a cut point that creates a reasonable number of clusters with a substantial number of observations per cluster (I know, “reasonable” and “substantial” are squishy terms). Below, cutting at height 0.65 to create 7 clusters seems good.\n\n\n# Inspect the tree to choose a size.\nplot(color_branches(as.dendrogram(mdl_hc), k = 7))\nabline(h = .65, col = \"red\")\n\n\n\n\n\n\n\n\n\n“Cut” the hierarchical tree into the desired number of clusters (k) or height h with cutree(hclust, k = NULL, h = NULL). cutree() returns a vector of cluster memberships. Attach this vector back to the original dataframe for visualization and summary statistics.\n\n\neap_2_clstr_hca &lt;- eap_2 %&gt;% mutate(cluster = cutree(mdl_hc, k = 7))\n\n\nCalculate summary statistics and draw conclusions. Useful summary statistics are typically membership count, and feature averages (or proportions).\n\n\neap_2_clstr_hca %&gt;% \n  group_by(cluster) %&gt;% \n  summarise_if(is.numeric, funs(mean(.)))\n\n# A tibble: 7 × 8\n  cluster EmployeeNumber MonthlyIncome YearsAtCompany YearsWithCurrManager\n    &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n1       1          1051.         8056.           8.21                 4.23\n2       2          1029.         5146.           5.98                 3.85\n3       3           974.         4450.           5.29                 3.51\n4       4          1088.         3961.           4.60                 2.64\n5       5           980.        16534.          14.2                  6.80\n6       6          1036.        12952.          14.9                  7.64\n7       7           993.        13733.          12.8                  6.53\n# ℹ 3 more variables: TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;,\n#   YearsInCurrentRole &lt;dbl&gt;\n\n\n\n1.3.0.1 K-Means vs HCA\nHierarchical clustering has some advantages over k-means. It can use any distance method - not just euclidean. The results are stable - k-means can produce different results each time. While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram. But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means. For this last reason, k-means is more common.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "01-cluster-analysis.html#footnotes",
    "href": "01-cluster-analysis.html#footnotes",
    "title": "1  Cluster Analysis",
    "section": "",
    "text": "Clustering is used to personalize employee experience. See A Complete Guide to the Employee Experience at AIHR.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "02-pca.html",
    "href": "02-pca.html",
    "title": "2  PCA",
    "section": "",
    "text": "2.1 Intuition\nShlens (2014) introduces PCA with a toy example where the movement of an oscillating spring is monitored by cameras from three positions in a room. Each camera produces a two-dimensional dataset of x and y coordinates, six vectors of numbers altogether. The physical reality is there is only one dimension of movement. PCA should be able to reduce the six vectors into a single principle component.\nI’ll simplify the example somewhat to make the math easier. I’ll locate the three cameras in the same spot, x = 0 and y = 0, and rotate them 15 degrees, 45 degrees, and 150 degrees respectively.\nShow the code\n# 100 photos of the end of the spring taken over 10s of motion.\nt &lt;- seq(0, 10, length.out = 100)\n\n# simple harmonic motion from our perspective: 0-degree rotation.\nx_0 &lt;- sin(2 * pi * 0.5 * t)\ny_0 &lt;- rnorm(length(t), 0, .03)\ncoords_0 &lt;- matrix(c(x_0, y_0), ncol = 2)\n\n# 15-degree rotation matrix for camera 1\ntheta_A = pi * (1 / 12)\nM_A &lt;- matrix(c(cos(theta_A), -sin(theta_A), sin(theta_A), cos(theta_A)), nrow = 2)\ncoords_A &lt;- coords_0 %*% M_A\n\n# 45-degree rotation matrix for camera 2\ntheta_B = pi * (1 / 4)\nM_B &lt;- matrix(c(cos(theta_B), -sin(theta_B), sin(theta_B), cos(theta_B)), nrow = 2)\ncoords_B &lt;- coords_0 %*% M_B\n\n# 150-degree rotation matrix for camera 3\ntheta_C = pi * (5 / 6)\nM_C &lt;- matrix(c(cos(theta_C), -sin(theta_C), sin(theta_C), cos(theta_C)), nrow = 2)\ncoords_C &lt;- coords_0 %*% M_C\n\n# Combined dataset\nspring &lt;- tibble(\n  t = t,\n  camera_A_x = coords_A[, 1],\n  camera_A_y = coords_A[, 2],\n  camera_B_x = coords_B[, 1],\n  camera_B_y = coords_B[, 2],\n  camera_C_x = coords_C[, 1],\n  camera_C_y = coords_C[, 2]\n)\nglimpse(spring)\n\nRows: 100\nColumns: 7\n$ t          &lt;dbl&gt; 0.0000000, 0.1010101, 0.2020202, 0.3030303, 0.4040404, 0.50…\n$ camera_A_x &lt;dbl&gt; -0.001465886, 0.305900249, 0.574032339, 0.791109832, 0.9078…\n$ camera_A_y &lt;dbl&gt; 0.0054707609, 0.0639693837, 0.1485024712, 0.1948177118, 0.3…\n$ camera_B_x &lt;dbl&gt; -0.004004875, 0.232932695, 0.422875353, 0.587712356, 0.6355…\n$ camera_B_y &lt;dbl&gt; 0.004004875, 0.208349236, 0.415623082, 0.564272003, 0.71486…\n$ camera_C_x &lt;dbl&gt; -0.002831874, -0.261537326, -0.510909264, -0.697156052, -0.…\n$ camera_C_y &lt;dbl&gt; -0.00490495, 0.17107096, 0.30089506, 0.42164220, 0.42889261…\nHere are the x-y coordinates recorded from the cameras’ perspectives..\nWe can perform a PCA in a single step. The center and scale. arguments are because PCA should operate on standardized data.\nspring_pca &lt;- \n  spring |&gt;\n  select(-t) |&gt;\n  prcomp(center = TRUE, scale. = TRUE)\n\nsummary(spring_pca)\n\nImportance of components:\n                          PC1     PC2       PC3     PC4      PC5       PC6\nStandard deviation     2.4426 0.18354 3.599e-16 1.1e-16 1.02e-16 9.594e-17\nProportion of Variance 0.9944 0.00561 0.000e+00 0.0e+00 0.00e+00 0.000e+00\nCumulative Proportion  0.9944 1.00000 1.000e+00 1.0e+00 1.00e+00 1.000e+00\nPCA creates as many components as there are variables. The summary table shows the proportion of the variance in the data set explained by each component. The standard deviation of the first component is 2.443 compared to nearly zero for the other five components. It explains essentially all of the dataset variance. The upshot is that you can replace the six columns in the dataset with just this one.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#intuition-and-matrix-algebra",
    "href": "02-pca.html#intuition-and-matrix-algebra",
    "title": "2  PCA",
    "section": "",
    "text": "Note on Orthonormal Bases\n\n\n\n\n\nAn orthonormal basis is any set of vectors whose pairwise inner products are zero. The orthonormal basis for camera \\(A\\) might be \\(\\{(\\sqrt{2}/2,\\sqrt{2}/2), (-\\sqrt{2}/2,\\sqrt{2}/2)\\}\\) from a neutral perspective. From the neutral perspective, you would have to rotate the axes 45 degrees.\n\n# M is a 45-degree line\nM &lt;- matrix(c(0:100, 0:100 * tan(45*(pi/180))), ncol = 2)\ncolnames(M) &lt;- c(\"X\", \"Y\")\n\n# M is unchanged after multiplication by identity matrix.\nI &lt;- matrix(c(1, 0, 0, 1), nrow = 2)\nM1 &lt;- M %*% I\ncolnames(M1) &lt;- colnames(M)\n\n# Rotate M 45-degrees.\nB &lt;- matrix(c(sqrt(2)/2, sqrt(2)/2, -sqrt(2)/2, sqrt(2)/2), nrow = 2)\nM2 &lt;- M %*% B\ncolnames(M2) &lt;- colnames(M)\n\nbind_rows(\n  `45 degree line` = as_tibble(M1),\n  `Rotated 45 degrees` = as_tibble(M2),\n  .id = \"series\"\n) |&gt;\n  ggplot(aes(x = X, y = Y, color = series)) +\n  geom_line() +\n  labs(x = NULL, y = NULL, color = NULL)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#case-study",
    "href": "02-pca.html#case-study",
    "title": "2  PCA",
    "section": "2.3 Case Study",
    "text": "2.3 Case Study\nLet’s work with a case study presented by Laerd. 315 job candidates complete a questionnaire consisting of 25 questions.\n\npca_dat &lt;- \n  foreign::read.spss(\"./input/pca.sav\", to.data.frame = TRUE) |&gt;\n  mutate(\n    across(where(is.factor), ~factor(., levels = likert_scale, ordered = TRUE))\n  )\n\nglimpse(pca_dat)\n\nRows: 315\nColumns: 30\n$ Qu1    &lt;ord&gt; Agree Somewhat, Disagree, Disagree Somewhat, Undecided, Undecid…\n$ Qu2    &lt;ord&gt; Undecided, Strongly Agree, Agree, Disagree, Disagree, Disagree,…\n$ Qu3    &lt;ord&gt; Undecided, Agree, Agree Somewhat, Agree, Agree, Agree Somewhat,…\n$ Qu4    &lt;ord&gt; Undecided, Agree, Agree, Disagree, Disagree, Agree Somewhat, Ag…\n$ Qu5    &lt;ord&gt; Undecided, Agree, Agree Somewhat, Disagree Somewhat, Disagree S…\n$ Qu6    &lt;ord&gt; Undecided, Agree Somewhat, Agree Somewhat, Disagree, Disagree, …\n$ Qu7    &lt;ord&gt; Undecided, Agree Somewhat, Agree, Disagree Somewhat, Disagree S…\n$ Qu8    &lt;ord&gt; Undecided, Agree Somewhat, Agree Somewhat, Disagree, Disagree, …\n$ Qu9    &lt;ord&gt; Agree Somewhat, Agree Somewhat, Disagree Somewhat, Undecided, U…\n$ Qu10   &lt;ord&gt; Undecided, Agree Somewhat, Disagree Somewhat, Disagree, Disagre…\n$ Qu11   &lt;ord&gt; Agree Somewhat, Agree Somewhat, Disagree Somewhat, Agree Somewh…\n$ Qu12   &lt;ord&gt; Strongly Agree, Agree, Agree, Agree, Agree, Agree, Agree, Agree…\n$ Qu13   &lt;ord&gt; Undecided, Agree, Agree, Strongly Agree, Strongly Agree, Agree,…\n$ Qu14   &lt;ord&gt; Undecided, Agree, Agree Somewhat, Agree Somewhat, Agree Somewha…\n$ Qu15   &lt;ord&gt; Undecided, Agree, Agree, Agree Somewhat, Agree Somewhat, Agree …\n$ Qu16   &lt;ord&gt; Undecided, Agree, Agree Somewhat, Undecided, Undecided, Disagre…\n$ Qu17   &lt;ord&gt; Undecided, Agree, Agree Somewhat, Undecided, Undecided, Agree S…\n$ Qu18   &lt;ord&gt; Undecided, Agree Somewhat, Agree, Agree Somewhat, Agree Somewha…\n$ Qu19   &lt;ord&gt; Agree Somewhat, Strongly Agree, Agree, Disagree, Disagree, Disa…\n$ Qu20   &lt;ord&gt; Undecided, Agree, Agree Somewhat, Disagree Somewhat, Disagree S…\n$ Qu21   &lt;ord&gt; Undecided, Strongly Agree, Agree Somewhat, Agree, Agree, Strong…\n$ Qu22   &lt;ord&gt; Undecided, Agree Somewhat, Agree Somewhat, Agree Somewhat, Agre…\n$ Qu23   &lt;ord&gt; Undecided, Strongly Agree, Agree Somewhat, Agree Somewhat, Agre…\n$ Qu24   &lt;ord&gt; Undecided, Agree, Agree Somewhat, Agree, Agree, Agree, Undecide…\n$ Qu25   &lt;ord&gt; Undecided, Strongly Agree, Agree Somewhat, Agree Somewhat, Agre…\n$ FAC1_1 &lt;dbl&gt; -0.31985907, -1.85802269, -1.36495416, 0.02235922, 0.02235922, …\n$ FAC2_1 &lt;dbl&gt; -0.6366505, -1.5727256, -1.6357254, -0.6846857, -0.6846857, -1.…\n$ FAC3_1 &lt;dbl&gt; 0.825818943, -1.576466472, -0.125140607, -0.576649155, -0.57664…\n$ FAC4_1 &lt;dbl&gt; -1.4492270, -0.9594769, 0.3063403, -0.3991595, -0.3991595, 1.60…\n$ FAC5_1 &lt;dbl&gt; -0.03960671, -0.03584510, -0.80707018, 1.59287496, 1.59287496, …\n\n\nThe questions focused on four attributes: motivation, dependability, enthusiasm, and commitment.\n\nMotivatonDependabilityEnthusiasmCommitment\n\n\n\ngt_items(q_motivation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nStrongly Agree N = 301\nAgree N = 1361\nAgree Somewhat N = 4061\nUndecided N = 7261\nDisagree Somewhat N = 7691\nDisagree N = 2061\nStrongly Disagree N = 2471\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Qu3\n1 (0.3%)\n19 (6.0%)\n54 (17%)\n97 (31%)\n94 (30%)\n27 (8.6%)\n23 (7.3%)\n\n\n    Qu4\n1 (0.3%)\n12 (3.8%)\n53 (17%)\n106 (34%)\n91 (29%)\n25 (7.9%)\n27 (8.6%)\n\n\n    Qu5\n1 (0.3%)\n19 (6.0%)\n61 (19%)\n88 (28%)\n102 (32%)\n23 (7.3%)\n21 (6.7%)\n\n\n    Qu6\n3 (1.0%)\n13 (4.1%)\n45 (14%)\n102 (32%)\n104 (33%)\n25 (7.9%)\n23 (7.3%)\n\n\n    Qu7\n4 (1.3%)\n11 (3.5%)\n42 (13%)\n99 (31%)\n90 (29%)\n37 (12%)\n32 (10%)\n\n\n    Qu8\n7 (2.2%)\n7 (2.2%)\n42 (13%)\n68 (22%)\n119 (38%)\n42 (13%)\n30 (9.5%)\n\n\n    Qu12\n1 (0.3%)\n26 (8.3%)\n44 (14%)\n76 (24%)\n83 (26%)\n16 (5.1%)\n69 (22%)\n\n\n    Qu13\n12 (3.8%)\n29 (9.2%)\n65 (21%)\n90 (29%)\n86 (27%)\n11 (3.5%)\n22 (7.0%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n\ngt_items(q_dependability)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nStrongly Agree N = 491\nAgree N = 1851\nAgree Somewhat N = 4301\nUndecided N = 6281\nDisagree Somewhat N = 5531\nDisagree N = 2671\nStrongly Disagree N = 931\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Qu2\n15 (4.8%)\n31 (9.8%)\n51 (16%)\n79 (25%)\n69 (22%)\n51 (16%)\n19 (6.0%)\n\n\n    Qu14\n2 (0.6%)\n31 (9.8%)\n53 (17%)\n101 (32%)\n85 (27%)\n30 (9.5%)\n13 (4.1%)\n\n\n    Qu15\n4 (1.3%)\n26 (8.3%)\n67 (21%)\n92 (29%)\n80 (25%)\n33 (10%)\n13 (4.1%)\n\n\n    Qu16\n7 (2.2%)\n25 (7.9%)\n66 (21%)\n77 (24%)\n90 (29%)\n39 (12%)\n11 (3.5%)\n\n\n    Qu17\n12 (3.8%)\n29 (9.2%)\n80 (25%)\n94 (30%)\n71 (23%)\n24 (7.6%)\n5 (1.6%)\n\n\n    Qu18\n5 (1.6%)\n33 (10%)\n69 (22%)\n97 (31%)\n73 (23%)\n25 (7.9%)\n13 (4.1%)\n\n\n    Qu19\n4 (1.3%)\n10 (3.2%)\n44 (14%)\n88 (28%)\n85 (27%)\n65 (21%)\n19 (6.0%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n\ngt_items(q_enthusiasm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nStrongly Agree N = 1071\nAgree N = 3081\nAgree Somewhat N = 5621\nUndecided N = 4921\nDisagree Somewhat N = 3721\nDisagree N = 281\nStrongly Disagree N = 211\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Qu20\n8 (2.5%)\n37 (12%)\n74 (23%)\n83 (26%)\n87 (28%)\n17 (5.4%)\n9 (2.9%)\n\n\n    Qu21\n20 (6.3%)\n67 (21%)\n103 (33%)\n63 (20%)\n60 (19%)\n0 (0%)\n2 (0.6%)\n\n\n    Qu22\n23 (7.3%)\n68 (22%)\n114 (36%)\n59 (19%)\n49 (16%)\n0 (0%)\n2 (0.6%)\n\n\n    Qu23\n16 (5.1%)\n46 (15%)\n87 (28%)\n84 (27%)\n74 (23%)\n2 (0.6%)\n6 (1.9%)\n\n\n    Qu24\n21 (6.7%)\n55 (17%)\n95 (30%)\n96 (30%)\n48 (15%)\n0 (0%)\n0 (0%)\n\n\n    Qu25\n19 (6.0%)\n35 (11%)\n89 (28%)\n107 (34%)\n54 (17%)\n9 (2.9%)\n2 (0.6%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n\ngt_items(q_commitment)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nStrongly Agree N = 251\nAgree N = 601\nAgree Somewhat N = 1041\nUndecided N = 1491\nDisagree Somewhat N = 4841\nDisagree N = 3221\nStrongly Disagree N = 1161\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Qu1\n7 (2.2%)\n10 (3.2%)\n20 (6.3%)\n39 (12%)\n114 (36%)\n94 (30%)\n31 (9.8%)\n\n\n    Qu9\n4 (1.3%)\n19 (6.0%)\n21 (6.7%)\n19 (6.0%)\n117 (37%)\n94 (30%)\n41 (13%)\n\n\n    Qu10\n9 (2.9%)\n9 (2.9%)\n25 (7.9%)\n29 (9.2%)\n129 (41%)\n82 (26%)\n32 (10%)\n\n\n    Qu11\n5 (1.6%)\n22 (7.0%)\n38 (12%)\n62 (20%)\n124 (39%)\n52 (17%)\n12 (3.8%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\n\n\n\nLet’s drop the unrelated FAC* cols. PCA only works with numerical values. Our questionnaire uses a 7-level scale, so we’re probably okay to treat that as numeric.\n\npca_numeric &lt;-\n  pca_dat |&gt;\n  select(starts_with(\"Qu\")) |&gt; \n  mutate(across(everything(), as.numeric))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#assumptions",
    "href": "02-pca.html#assumptions",
    "title": "2  PCA",
    "section": "2.4 Assumptions",
    "text": "2.4 Assumptions\nMissing values can bias PCA results. If you have missing values, you should either impute, remove the observations, or drop the columns. Fortunately there are no missing values in this dataset.\n\ncolSums(is.na(pca_numeric))\n\n Qu1  Qu2  Qu3  Qu4  Qu5  Qu6  Qu7  Qu8  Qu9 Qu10 Qu11 Qu12 Qu13 Qu14 Qu15 Qu16 \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \nQu17 Qu18 Qu19 Qu20 Qu21 Qu22 Qu23 Qu24 Qu25 \n   0    0    0    0    0    0    0    0    0 \n\n\nThere are a few other assumptions to be aware of:\n\nPCA is based on correlation coefficients, so all variables should be linearly related (r &gt; .3) to at least one other variable.\nThe next assumption is that the sample sizes should be large. As a rule of thumb, there should be at least 5 cases per variable.\nThere should be no outliers. Component scores greater than 3 standard deviations away from the mean can have a disproportionate influence on the results.\n\nI found two procedures used to test assumptions (though it does not test all of them). They both operate on the correlation matrix.\n\ncorr_mtrx &lt;- cor(pca_numeric)\n\nThe first assumption could be tested by querying the matrix for the max correlations. The worst is q20.\n\n\nShow the code\n# ggcorrplot::ggcorrplot(corr_mtrx)\n\nmax_cor &lt;-\n  as_tibble(corr_mtrx) |&gt;\n  mutate(var1 = factor(q_colnames, levels = q_colnames)) |&gt;\n  pivot_longer(cols = c(Qu1:Qu25), names_to = \"var2\", values_to = \"rho\") |&gt;\n  filter(var1 != var2) |&gt;\n  slice_max(by = var1, order_by = rho, n = 1)\n\nmax_cor_v &lt;- round(max_cor$rho, 2)\nnames(max_cor_v) &lt;- max_cor$var1\nmax_cor_v\n\n\n Qu1  Qu2  Qu3  Qu4  Qu5  Qu6  Qu7  Qu8  Qu9 Qu10 Qu11 Qu12 Qu13 Qu14 Qu15 Qu16 \n0.58 0.68 0.69 0.68 0.53 0.56 0.63 0.63 0.55 0.60 0.60 0.62 0.69 0.80 0.80 0.59 \nQu17 Qu18 Qu19 Qu20 Qu21 Qu22 Qu23 Qu24 Qu25 \n0.63 0.63 0.68 0.38 0.69 0.69 0.59 0.62 0.62 \n\n\nThe Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy compares the variable correlations to the partial correlations in the data. The test measures sampling adequacy for each variable in the model and for the complete model.2\n\\[\n\\text{KMO}_j = \\frac{\\sum_{i \\ne j}r_{ij}^2}{\\sum_{i \\ne j}r_{ij}^2 + \\sum_{i \\ne j} u}\n\\]\nwhere \\(r_{ij}\\) are correlations, and \\(u_{ij}\\) are partial covariances. Scores range from 0 to 1. Values should be at least .6 to justify a PCA. Values over .8 are preferable.\n\nEFAtools::KMO(corr_mtrx)\n\n\n── Kaiser-Meyer-Olkin criterion (KMO) ──────────────────────────────────────────\n\n✔ The overall KMO value for your data is meritorious.\n  These data are probably suitable for factor analysis.\n\n  Overall: 0.833\n\n  For each variable:\n  Qu1   Qu2   Qu3   Qu4   Qu5   Qu6   Qu7   Qu8   Qu9  Qu10  Qu11  Qu12  Qu13 \n0.817 0.740 0.854 0.889 0.909 0.906 0.843 0.829 0.849 0.762 0.744 0.899 0.857 \n Qu14  Qu15  Qu16  Qu17  Qu18  Qu19  Qu20  Qu21  Qu22  Qu23  Qu24  Qu25 \n0.824 0.877 0.913 0.880 0.794 0.803 0.834 0.725 0.724 0.831 0.741 0.803 \n\n\nBartlett’s test of sphericity tests the null hypothesis that the correlation matrix is an identity matrix, i.e., there are no correlations between any variables.\n\nEFAtools::BARTLETT(corr_mtrx, N = nrow(pca_dat))\n\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(300) = 4214.06, p &lt; .001",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#perform-pca",
    "href": "02-pca.html#perform-pca",
    "title": "2  PCA",
    "section": "2.5 Perform PCA",
    "text": "2.5 Perform PCA\nThere are two stats package functions for PCA, princomp() and prcomp(). The documentation suggests prcomp() is preferable.\n\npca_result &lt;- prcomp(pca_numeric, center = TRUE, scale. = TRUE)\n\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6    PC7\nStandard deviation     2.5942 1.8282 1.7022 1.4204 1.02418 0.97525 0.9460\nProportion of Variance 0.2692 0.1337 0.1159 0.0807 0.04196 0.03804 0.0358\nCumulative Proportion  0.2692 0.4029 0.5188 0.5995 0.64145 0.67949 0.7153\n                           PC8    PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.87741 0.8306 0.79657 0.74071 0.71685 0.71026 0.67118\nProportion of Variance 0.03079 0.0276 0.02538 0.02195 0.02056 0.02018 0.01802\nCumulative Proportion  0.74608 0.7737 0.79906 0.82101 0.84156 0.86174 0.87976\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.61776 0.60252 0.59119 0.56626 0.54597 0.5315 0.50522\nProportion of Variance 0.01527 0.01452 0.01398 0.01283 0.01192 0.0113 0.01021\nCumulative Proportion  0.89502 0.90955 0.92353 0.93635 0.94828 0.9596 0.96978\n                          PC22    PC23    PC24    PC25\nStandard deviation     0.48827 0.45536 0.41360 0.37230\nProportion of Variance 0.00954 0.00829 0.00684 0.00554\nCumulative Proportion  0.97932 0.98761 0.99446 1.00000\n\n\nThe top 4 components account for 60% of the variance. Subsequent components add less than 4% of explanatory power. The component loadings show how they relate to each column. Loadings are the eigenvectors of the covariance matrix. They are in the rotatation object returned by prcomp(). You should scale them by standard deviations.\n\npca_loadings &lt;- pca_result$rotation %*% diag(pca_result$sdev)\n\nHere are the loadings from the top 8 components.\n\n\nShow the code\ncolnames(pca_loadings) &lt;- paste0(\"pc\", 1:25)\n\npca_loadings |&gt;\n  as_tibble(rownames = \"Q\") |&gt;\n  mutate(Area = case_when(\n    Q %in% q_motivation ~ \"Motivation\",\n    Q %in% q_dependability ~ \"Dependability\",\n    Q %in% q_enthusiasm ~ \"Enthusiasm\",\n    Q %in% q_commitment ~ \"Commitment\"\n  )) |&gt;\n  select(Area, Q, pc1:pc8) |&gt;\n  arrange(Area, Q) |&gt;\n  gt::gt() |&gt; \n  gt::fmt_number(pc1:pc8, decimals = 3) |&gt;\n  gt::data_color(\n    columns = pc1:pc8, \n    method = \"numeric\",\n    palette = c(\"firebrick\", \"white\", \"dodgerblue3\"),\n    domain = c(-1, 1)\n  ) |&gt;\n  gt::tab_row_group(label = \"Commitment\", rows = Area == \"Commitment\") |&gt;\n  gt::tab_row_group(label = \"Enthusiasm\", rows = Area == \"Enthusiasm\") |&gt;\n  gt::tab_row_group(label = \"Dependability\", rows = Area == \"Dependability\") |&gt;\n  gt::tab_row_group(label = \"Motivation\", rows = Area == \"Motivation\") |&gt;\n  gt::cols_hide(columns = Area) |&gt;\n  gt::tab_header(\"Factor loadings by survey area.\") |&gt;\n  gt::tab_options(heading.align = \"left\")\n\n\n\n\n\n\n\n\nFactor loadings by survey area.\n\n\nQ\npc1\npc2\npc3\npc4\npc5\npc6\npc7\npc8\n\n\n\n\nMotivation\n\n\nQu12\n0.656\n0.282\n−0.046\n−0.343\n−0.142\n−0.172\n0.089\n−0.026\n\n\nQu13\n0.642\n0.265\n−0.175\n−0.347\n−0.205\n−0.052\n−0.155\n0.101\n\n\nQu3\n0.671\n0.286\n−0.217\n−0.265\n−0.279\n0.036\n−0.134\n0.197\n\n\nQu4\n0.677\n0.203\n−0.274\n−0.287\n−0.124\n−0.010\n−0.004\n−0.090\n\n\nQu5\n0.586\n0.203\n−0.100\n−0.276\n−0.043\n−0.123\n−0.232\n−0.082\n\n\nQu6\n0.652\n0.296\n−0.052\n−0.290\n0.061\n0.023\n−0.064\n−0.169\n\n\nQu7\n0.559\n0.332\n−0.001\n−0.223\n0.421\n0.143\n0.353\n0.096\n\n\nQu8\n0.527\n0.375\n−0.181\n−0.094\n0.473\n0.225\n0.273\n−0.088\n\n\nDependability\n\n\nQu14\n0.649\n−0.295\n0.444\n0.065\n−0.250\n0.046\n0.108\n0.132\n\n\nQu15\n0.668\n−0.333\n0.405\n0.062\n−0.123\n−0.015\n0.061\n0.227\n\n\nQu16\n0.621\n−0.242\n0.349\n0.071\n0.204\n−0.125\n−0.103\n0.096\n\n\nQu17\n0.561\n−0.401\n0.349\n0.085\n0.158\n0.078\n−0.208\n0.253\n\n\nQu18\n0.617\n−0.289\n0.355\n0.110\n0.292\n−0.082\n−0.056\n0.186\n\n\nQu19\n0.579\n−0.185\n0.473\n0.221\n0.010\n0.100\n0.049\n−0.371\n\n\nQu2\n0.541\n−0.203\n0.414\n0.192\n−0.328\n0.193\n0.150\n−0.409\n\n\nEnthusiasm\n\n\nQu20\n0.357\n−0.229\n−0.335\n0.089\n0.165\n0.432\n−0.556\n−0.191\n\n\nQu21\n0.244\n−0.457\n−0.583\n0.043\n−0.107\n0.366\n0.154\n0.148\n\n\nQu22\n0.269\n−0.398\n−0.579\n0.132\n−0.228\n0.246\n0.318\n0.118\n\n\nQu23\n0.505\n−0.306\n−0.422\n0.275\n−0.004\n−0.366\n0.106\n−0.135\n\n\nQu24\n0.253\n−0.498\n−0.573\n0.139\n0.069\n−0.309\n−0.085\n−0.005\n\n\nQu25\n0.423\n−0.428\n−0.458\n0.224\n0.179\n−0.270\n0.051\n−0.128\n\n\nCommitment\n\n\nQu1\n0.230\n0.540\n−0.081\n0.545\n−0.033\n−0.014\n−0.048\n0.228\n\n\nQu10\n0.335\n0.574\n−0.161\n0.535\n0.034\n−0.085\n−0.070\n−0.089\n\n\nQu11\n0.153\n0.479\n−0.147\n0.619\n−0.064\n0.207\n−0.114\n0.111\n\n\nQu9\n0.314\n0.564\n0.123\n0.391\n−0.127\n−0.136\n0.125\n−0.001\n\n\n\n\n\n\n\n\nbiplot(pca_result, scale = 0)\n\n\n\n\n\n\n\npca_result$sdev^2 / sum(pca_result$sdev^2)\n\n [1] 0.269193743 0.133693712 0.115900948 0.080701891 0.041958016 0.038044232\n [7] 0.035795413 0.030794254 0.027597268 0.025380915 0.021945950 0.020555035\n[13] 0.020178720 0.018019257 0.015265125 0.014521264 0.013980218 0.012825942\n[19] 0.011923156 0.011297921 0.010209762 0.009536248 0.008293999 0.006842642\n[25] 0.005544370\n\n\nThe communality is the proportion of each variable’s variance that is accounted for by the principal components analysis and can also be expressed as a percentage.\nA principal components analysis will produce as many components as there are variables. However, the purpose of principal components analysis is to explain as much of the variance in your variables as possible using as few components as possible. After you have extracted your components, there are four major criteria that can help you decide on the number of components to retain: (a) the eigenvalue-one criterion, (b) the proportion of total variance accounted for, (c) the scree plot test, and (d) the interpretability criterion. All except for the first criterion will require some degree of subjective analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#appendix-eigenvectors",
    "href": "02-pca.html#appendix-eigenvectors",
    "title": "2  PCA",
    "section": "2.6 Appendix: Eigenvectors",
    "text": "2.6 Appendix: Eigenvectors\nA square matrix, \\(\\textbf{A}\\), can be decomposed into eigenvalues, \\(\\lambda\\), and eigenvectors, \\(\\textbf{v}\\).3\n\\[\n\\textbf{A} \\textbf{v} = \\lambda \\textbf{v}\n\\tag{2.1}\\]\nFor example, \\(6\\) and \\(\\begin{bmatrix}1 \\\\ 4 \\end{bmatrix}\\) are an eigenvalue and eigenvector here:\n\\[\n\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix}1 \\\\ 4 \\end{bmatrix} = 6 \\begin{bmatrix}1 \\\\ 4 \\end{bmatrix}\n\\]\nEquation 2.1 can be re-expressed as \\(\\textbf{A} \\textbf{v} - \\lambda \\textbf{I} \\textbf{v} = 0.\\) For \\(\\textbf{v}\\) to be non-zero, the determinant must be zero,\n\\[\n| \\textbf{A} - \\lambda \\textbf{I}| = 0\n\\tag{2.2}\\]\nBack to the example, use Equation 2.2 to find possible eigenvalues.\n\\[\n\\left| \\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\right| = 0\n\\]\nSubtract the matrices and calculate the determinant, \\((-6 - \\lambda)(5 - \\lambda) - 3 \\times 4 = 0,\\) then solve for \\(\\lambda = -7 \\text{ or } 6.\\) Now that you have the possible eigenvalues, plug them back into Equation @ref(eq:eigen1). For \\(\\lambda = 6\\) you have\n\\[\n\\begin{bmatrix} -6 & 3 \\\\ 4 & 5 \\end{bmatrix} \\begin{bmatrix}x \\\\ y \\end{bmatrix} = 6 \\begin{bmatrix}x \\\\ y \\end{bmatrix}\n\\]\nSolving the system of equations reveals that \\(y = 4x\\). So \\(\\begin{bmatrix}1 \\\\ 4 \\end{bmatrix}\\) is a solution. You can do the same exercise for \\(\\lambda = -7\\).\nEigenvectors and eigenvalues are useful because matrices are used to make transformations in space. In transformations, the eigenvector is the axis of rotation, the direction that does not change, and the eigenvalue is the scale of the stretch (1 = no change, 2 = double length, -1 = point backwards, etc.).\n\n\n\n\n\n\nShlens, Jonathon. 2014. “A Tutorial on Principal Component Analysis.” arXiv Preprint arXiv:1404.1100. https://arxiv.org/abs/1404.1100.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "02-pca.html#footnotes",
    "href": "02-pca.html#footnotes",
    "title": "2  PCA",
    "section": "",
    "text": "Material from PSU, Laerd, and Shlens (2014).↩︎\nSee Statistics How-to.↩︎\nTook these notes from Math is Fun.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  },
  {
    "objectID": "03-references.html",
    "href": "03-references.html",
    "title": "References",
    "section": "",
    "text": "Shlens, Jonathon. 2014. “A Tutorial on Principal Component\nAnalysis.” arXiv Preprint arXiv:1404.1100. https://arxiv.org/abs/1404.1100.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "02-pca.html#matrix-algebra",
    "href": "02-pca.html#matrix-algebra",
    "title": "2  PCA",
    "section": "2.2 Matrix Algebra",
    "text": "2.2 Matrix Algebra\nPCA works by identifying the most meaningful basis to re-express the dataset. In our toy example, the unit basis along the x-axis is the most meaningful. It does this with matrix algebra.\nEach row of the dataset may be expressed as a column vector, \\(\\vec{X}\\) that is the x and y values recorded by the three cameras.\n\\[\n\\vec{X} = \\begin{bmatrix} A_x \\\\ A_y \\\\ B_x \\\\ B_y \\\\ C_x \\\\ C_y \\end{bmatrix}\n\\]\n\\(\\vec{X}\\) lies in an m=6-dimensional vector space spanned by an orthonormal basis. The orthonormal bases for each camera are \\(\\{(1,0), (0,1)\\}\\) from their own perspective, so a data point from camera \\(A\\) might equivalently be expressed as\n\\[\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} A_x \\\\ A_y \\end{bmatrix}\n\\]\n\n\n\n\n\n\nNote on Orthonormal Bases\n\n\n\n\n\nAn orthonormal basis is any set of vectors whose pairwise inner products are zero. The orthonormal basis for camera \\(A\\) might be \\(\\{(\\sqrt{2}/2,\\sqrt{2}/2), (-\\sqrt{2}/2,\\sqrt{2}/2)\\}\\) from a neutral perspective. From the neutral perspective, you would have to rotate the axes 45 degrees.\n\n# M is a 45-degree line\nM &lt;- matrix(c(0:100, 0:100 * tan(45*(pi/180))), ncol = 2)\ncolnames(M) &lt;- c(\"X\", \"Y\")\n\n# M is unchanged after multiplication by identity matrix.\nI &lt;- matrix(c(1, 0, 0, 1), nrow = 2)\nM1 &lt;- M %*% I\ncolnames(M1) &lt;- colnames(M)\n\n# Rotate M 45-degrees.\nB &lt;- matrix(c(sqrt(2)/2, sqrt(2)/2, -sqrt(2)/2, sqrt(2)/2), nrow = 2)\nM2 &lt;- M %*% B\ncolnames(M2) &lt;- colnames(M)\n\nbind_rows(\n  `45 degree line` = as_tibble(M1),\n  `Rotated 45 degrees` = as_tibble(M2),\n  .id = \"series\"\n) |&gt;\n  ggplot(aes(x = X, y = Y, color = series)) +\n  geom_line() +\n  labs(x = NULL, y = NULL, color = NULL)\n\n\n\n\n\n\n\n\n\n\n\nExtending this to all three cameras, you might start by naively assuming they collect data from the same perspective, taking each measurement at face value. The set of orthonormal basis vectors, \\(\\textbf{B}\\), would look like this identity matrix\n\\[\n\\textbf{B} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{bmatrix}\n\\]\nwhere \\(b_1\\) and \\(b_2\\) are the bases used by camera \\(A\\), etc. Now the data set \\(\\textbf{X}\\) can be expressed as the matrix multiplication, \\(\\textbf{BX}\\). This added complexity allows you to ask whether another basis, \\(\\textbf{P}\\), that is a linear combination of the original basis, better expresses the data set, \\(\\textbf{PX} = \\textbf{Y}\\). The linear restriction is a key simplifying assumption of PCA. \\(\\textbf{P}\\) transforms \\(\\textbf{X}\\) into \\(\\textbf{Y}\\), but you can also think of it as rotating and stretching \\(\\textbf{X}\\) into \\(\\textbf{Y}.\\)\nSo, what transformation is best? The ideal is to maximize variance (signal-to-noise ratio) and minimize covariance (redundancy) in the data. The signal-to-noise ratio is \\(SNR = \\sigma^2_{signal} / \\sigma^2_{noise}\\). In each camera’s 2D perspective, the signal is the amplitude of the movement of the spring along the x-axis and the noise is the movement along the y-axis. SNR is maximized by rotating the camera axes (gold below).\n\n\n\n\n\n\n\n\n\nThe other criteria is redundancy. The three cameras record the same activity, so they are highly correlated.\n\nspring |&gt; select(ends_with(\"x\")) |&gt; GGally::ggpairs()\n\n\n\n\n\n\n\n\nIf \\(\\textbf{X}\\) is an \\(m \\times n\\) matrix of centered values (subtracting the mean), PCA finds an orthonormal matrix \\(\\textbf{P}\\) in \\(\\textbf{Y} = \\textbf{PX}\\) such that the covariance matrix \\(\\textbf{C}_\\textbf{Y} = \\frac{1}{n}\\textbf{YY}^T\\) is diagonal. The rows of \\(\\textbf{P}\\) are the principal components of \\(\\textbf{X}\\). The off-diagonal elements of a covariance matrix are the covariances and they measure redundancy. The diagonal elements are the variances and they measure signal.\nWith matrix algebra, you can express \\(\\textbf{C}_\\textbf{Y}\\) in terms of the covariance matrix of \\(\\textbf{X}\\), \\(\\textbf{C}_\\textbf{X}\\).\n\\[\n\\begin{align}\n\\textbf{C}_\\textbf{Y} &= \\frac{1}{n}\\textbf{YY}^T \\\\\n&= \\frac{1}{n}(\\textbf{PX})(\\textbf{PX})^T \\\\\n&= \\frac{1}{n}\\textbf{PXX}^T\\textbf{P}^T \\\\\n&= P\\left(\\frac{1}{n}\\textbf{XX}^T\\right)\\textbf{P}^T \\\\\n&= \\textbf{PC}_\\textbf{X}\\textbf{P}^T\n\\end{align}\n\\]\nAny symmetric matrix can be diagonalized by an orthogonal matrix of its eigenvectors, \\(\\textbf{C}_\\textbf{X} = \\textbf{E}^T\\textbf{DE}\\). So select \\(\\textbf{P}\\) to be such a matrix.\n\\[\n\\begin{align}\n\\textbf{C}_\\textbf{Y} &= \\textbf{PC}_\\textbf{X}\\textbf{P}^T \\\\\n&= \\textbf{P}(\\textbf{E}^T\\textbf{DE})\\textbf{P}^T \\\\\n&= \\textbf{P}(\\textbf{P}^T\\textbf{DP})\\textbf{P}^T \\\\\n&= (\\textbf{PP}^T)\\textbf{D}(\\textbf{PP}^T) \\\\\n&= (\\textbf{PP}^{-1})\\textbf{D}(\\textbf{PP}^{-1}) \\\\\n&= \\textbf{D}\n\\end{align}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>PCA</span>"
    ]
  }
]